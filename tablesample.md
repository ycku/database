# TABLESAMPLE

本文要探討的問題是：**「從資料庫得到想要的結果，一定要計算完所有資料嗎？」**

這個問題的關鍵點在於，**你是否知道所謂想要的結果是什麼？**  
我們時常抱怨資料庫處理的速度太慢，尤其在資料量不斷增加的發展下，對時間的要求卻只有更嚴格。具體來說，如果你想知道過去一個月內在系統記錄中，CPU 的平均負載情況如何？要求是每分鐘回報一次。而很不幸地，測試結果每 100 萬筆資料，需要 80 秒的運算時間，而組織要求的系統記錄筆數越來越多。

在硬體資源不變的條件下，我們有兩個選擇，一個是對於查詢演算法鑽牛角尖，更聰明地精確計算每一個數字；另一個是本文要說明的方案，採用大量抽樣的方式，控制固定的運算量，只要能夠**誤差在實務上可接受**就好了。

機率統計的學理不在此處探討，本文只是提出一個被大家遺忘的出路罷了。回到舉例的問題，單次運算時間超過要求的時間，新的查詢方法、新的程式架構、新的演算法，成本難度都不低；但如果回頭想想，標的是 CPU 負載，如果可接受的結果是，50%、55%、60%，這樣的數字，而非像是 52.345791% 這樣的精確結果，有可能只需要一半不到的運算時間就達到了。這在運算上沒有難度，學理上也已有基礎，只有一個限制：**你清楚知道你的資料需求嗎？**

所謂可接受的誤差，其實最重要的是來自於對實務資料需求的掌握度。所以我們並非是犧牲精確度來解決問題，而是**增加資料需求掌握度**來解決問題。

這個思維有幾個好處：

1. 減少運算資源浪費：你只運算到你需要的程度。
2. 增加運算效率：你可以更快得到你需要的結果。
3. 降低系統成本：較少的運算負載，你可以使用輕量級的設備。
4. 增加系統使用率：對資料庫更多短查詢，更容易調配共享資源。

